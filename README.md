## Курсовой проект по теме:  
### "Разработка, развёртывание, настройка мониторинга и пайплайна переобучения модели бинарной классификации с использованием инструментов Big Data."  

Некоторый код для проекта был взят из домашних заданий, в которых делались похожие операции, но на другом датасете. Ссылка на репозиторий с ДЗ [здесь](https://github.com/mikhail-rozov/Otus-MLOps-course).  

Ссылка на презентацию к проекту - [Презентация](https://github.com/mikhail-rozov/otus-course-project/blob/master/Presentation.pptx)  

Ниже представлены некоторые разделы из презентации:

### <ins>1. Постановка задачи</ins>  
В данном курсовом проекте требовалось спроектировать автоматизированный ML-пайплайн, включающий в себя разработку ML-модели, её развёртывание в среде Kubernetes с масштабированием под нагрузкой, настройку мониторинга и периодическое обновление модели на новых данных. При этом нужно было максимально использовать облачную инфраструктуру и распределённые вычисления и основной упор сделать на развёртывании модели и CI/CD-процессах, а не на качестве получаемых предсказаний.  

### <ins>2. Конвейер предобработки данных</ins> 

Данные для выполнения задачи были взяты с платформы Kaggle (https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection/data). Это классический табличный датасет для бинарной классификации, где целевой переменной является факт того, что пользователь скачал приложение после клика по рекламному баннеру. Датасет состоит из train и test частей. Train-часть делилась на два датасета: train (700 тыс. записей) и test (80 тыс. записей), а исходная test-часть использовалась для инференса модели и создания нагрузки на кластер Kubernetes.
За основу кода предобработки был взят пример из ноутбуков Kaggle, при этом было принято решение делать предобработку на облачном Spark-кластере, поэтому код из Pandas был переработан для работы в PySpark.
Всего получилось 2 скрипта обработки данных ([data_preparation_initial.py](https://github.com/mikhail-rozov/otus-course-project/blob/master/data_preparation_initial.py) и [data_preparation.py](https://github.com/mikhail-rozov/otus-course-project/blob/master/data_preparation.py)): в одном создаётся начальный train-датасет и тестовый датасет для проверки качества модели, а другой используется для обработки новых данных и добавления их к имеющемуся обучающему набору. По расписанию запускался только второй скрипт.
Данные хранились в S3-бакете, как необработанные, так и после обработки.

### <ins>3. Выбор модели. Метрики.</ins> 
Так как данный курсовой проект не накладывает серьёзных требований к разработке и качеству модели, то изначально планировалось использовать только логистическую регрессию, но по причинам, описанным далее в презентации, была также использована модель случайного леса, которая оказалась лучше по метрике.
В качестве метрики была взята ROC-AUC, без какого-то глубокого анализа, просто, чтобы было, как сравнивать модели. При этом рассчитывалось среднее значение интервала метрик на 100 бутстрап-выборках. Ниже представлен скриншот с результатом логистической регрессии из MLflow:  

<img width="1051" height="403" alt="Image" src="https://github.com/user-attachments/assets/84849105-f58c-4838-8921-c3e22c547baf" />

### <ins>4. Инфраструктура</ins> 
Для данной задачи использовалось Яндекс-облако, в котором были подняты 3 виртуальные машины (Airflow, MLflow и postgres для него), а также Kafka-кластер, который использовался для инференса модели. В процессе предобработки данных Airflow также поднимал облачный Spark-кластер из 4 нод, который затем удалялся. Kubernetes-кластер поднимался локально с использованием Kind. Скрипт имитации поступления новых данных в Kafka на инференс модели запускался локально (kafka_producer.py). Docker-образ с моделью хранился в DockerHub.
Облачная инфраструктура контролировалась через интерфейс облака Яндекса, локальный скрипт контролировался через терминал, а Kubernetes-кластер – через графический интерфейс Lens.

### <ins>5. CI/CD-конвейер</ins>
Конвейер CI/CD состоит из оркестратора Airflow, функционала Github Actions и Kubernetes.
Скриншот структуры DAG’а Airflow:  

<img width="2364" height="549" alt="Image" src="https://github.com/user-attachments/assets/1079b634-038a-4d6a-b805-f18080d8df80" />  

Раз в сутки Airflow ([скрипт](https://github.com/mikhail-rozov/otus-course-project/blob/master/airflow_script.py)) поднимает Spark-кластер, берёт новые данные из S3-бакета, предобрабатывает их, затем удаляет кластер и запускает [скрипт обучения модели](https://github.com/mikhail-rozov/otus-course-project/blob/master/model_refitting.py).
Новая модель сравнивается по метрике с предыдущей с помощью двухвыборочного t-теста. При статистически значимом увеличении средней roc-auc (альфа=0,05), новая модель принимается.  

Скриншот метрики новой модели и p-value двухвыборочного теста:  

<img width="1051" height="403" alt="Image" src="https://github.com/user-attachments/assets/d73d9c63-f646-4ae6-8a69-adbf2f0f61b7" />  

Околонулевой p-value говорит о том, что разница между метриками статистически значима, поэтому новая модель автоматически пушится в github.
При пуше в github в работу вступает [Github Actions](https://github.com/mikhail-rozov/otus-course-project/blob/master/.github/workflows/docker.yml), который собирает docker-образ из модели и пушит его в Dockerhub.

### <ins>6. Инференс модели</ins>  

На базе образа лучшей модели в Dockerhub создаются поды кластера Kubernetes:  

<img width="3244" height="609" alt="Image" src="https://github.com/user-attachments/assets/a8e07865-8b8a-495b-b52c-a1f5f485166f" />  

В подах запускается скрипт [model_inference.py](https://github.com/mikhail-rozov/otus-course-project/blob/master/model_inference.py), который слушает input топик Kafka. При получении данных он производит их обработку и инференс. Предсказания отправляются в predictions топик Kafka.  
В кластере Kubernetes реализовано автоматическое горизонтальное масштабирование HPA. При высокой нагрузке число подов автоматически повышается с 3-х до 6-ти.  

<img width="3244" height="609" alt="Image" src="https://github.com/user-attachments/assets/b2ed9e86-2c42-4b54-8815-9b5c970bd9d8" />  

На скриншоте видно, что нагрузка на ЦПУ выросла, поэтому кластер стал увеличивать число реплик:  

<img width="3244" height="609" alt="Image" src="https://github.com/user-attachments/assets/2a2ecc2b-6b9c-497b-95ec-051d6ce72f1a" />  

Манифесты Kubernetes - [тут](https://github.com/mikhail-rozov/otus-course-project/tree/master/k8s)
